{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "339340f1-1bb6-4e87-bc27-ae0bda8bfbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "pd.options.display.max_rows = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ec97c78-1818-4caa-8914-953848c99872",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'raw_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871e7f5a-f0e5-46e9-b3fa-3590bb2da9e8",
   "metadata": {},
   "source": [
    "## load raw data processed using matlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03b70e66-e5ee-4f61-be45-7db1e286be86",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = os.listdir(data_dir)\n",
    "csv_files = [file for file in csv_files if file.endswith('.csv')]\n",
    "csv_files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51cfb3e6-2cd3-4198-85f0-af5820fc72b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load label file\n",
    "label_file = open(\"processed_data/label_dict.pkl\", \"rb\")\n",
    "label_dict = pickle.load(label_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "980992d9-5a16-44f1-a370-8f874f01b9a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "participant_10_10_32.csv\n",
      "participant_13_12_32.csv\n",
      "participant_14_2_74.csv\n",
      "participant_15_27_32.csv\n",
      "participant_16_22_32.csv\n",
      "participant_17_1_62.csv\n",
      "participant_18_8_32.csv\n",
      "participant_1_1_69.csv\n",
      "participant_1_71_32.csv\n",
      "participant_22_1_15.csv\n",
      "participant_25_11_32.csv\n",
      "participant_28_10_32.csv\n",
      "participant_2_7_32.csv\n",
      "participant_32_1_36.csv\n",
      "participant_33_36_32.csv\n",
      "participant_35_1_58.csv\n",
      "participant_37_1_68.csv\n",
      "participant_3_1_55.csv\n",
      "participant_42_1_34.csv\n",
      "participant_45_1_13.csv\n",
      "participant_48_10_32.csv\n",
      "participant_4_21_32.csv\n",
      "participant_52_1_20.csv\n",
      "participant_53_11_32.csv\n",
      "participant_55_1_25.csv\n",
      "participant_57_1_63.csv\n",
      "participant_58_1_42.csv\n",
      "participant_59_2_20.csv\n",
      "participant_60_1_61.csv\n",
      "participant_63_18_32.csv\n",
      "participant_65_1_26.csv\n",
      "participant_67_1_74.csv\n",
      "participant_69_21_32.csv\n",
      "participant_70_2_23.csv\n",
      "participant_73_15_32.csv\n",
      "participant_75_2_23.csv\n",
      "participant_77_1_45.csv\n",
      "participant_79_2_60.csv\n",
      "participant_7_36_32.csv\n",
      "participant_80_2_12.csv\n",
      "participant_81_5_32.csv\n",
      "participant_83_2_60.csv\n",
      "participant_86_24_32.csv\n",
      "participant_88_1_32.csv\n",
      "participant_89_43_32.csv\n",
      "participant_91_2_61.csv\n",
      "participant_94_1_73.csv\n",
      "Complete loading\n"
     ]
    }
   ],
   "source": [
    "df_lists = []\n",
    "for idx, file in enumerate(csv_files):\n",
    "    if idx % 100 ==0:\n",
    "        print(file)\n",
    "    participant_file = pd.read_csv(data_dir+file,header=None)\n",
    "    # Drop time stamp\n",
    "    participant_file = participant_file.drop([0,4],axis=1)\n",
    "\n",
    "    column_names= ['acc_x','acc_y','acc_z','gy_x','gy_y','gy_z','user_id','recording_idx','exercise_idx']\n",
    "    participant_file.columns = column_names\n",
    "    participant_file['global_file_idx'] = idx\n",
    "    participant_file['label'] = participant_file['exercise_idx'].map(label_dict)\n",
    "    df_lists.append(participant_file)\n",
    "    \n",
    "print('Complete loading')\n",
    "# concat all files\n",
    "total_file = pd.concat(df_lists,axis=0)\n",
    "total_file = total_file.sort_values(by=['user_id','recording_idx','exercise_idx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e14a216-f497-4fd7-b33e-30446db635ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_file.to_csv('processed_data/total_file.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffbd877-b194-47d8-a877-a1769b576b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_file = pd.read_csv('processed_data/total_file.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4439b1e1-e255-4cd5-bd85-dffae432ae86",
   "metadata": {
    "tags": []
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1cb79c-aa7d-49ff-98f5-07b2f0932fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total of {total_file.global_file_idx.max()} activity files\")\n",
    "print(f\"Total of {len(total_file.label.unique())} unique activity files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6603ce2-2f5f-4681-847b-748836725924",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Value counts for each label\")\n",
    "value_counts_by_exercise = dict(total_file.groupby('global_file_idx').head(1)['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da93b009-eabd-4c50-8e11-d642580ff9dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "value_counts_by_exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436cac15-9f2f-4e64-bb3f-1beeb5af1202",
   "metadata": {
    "tags": []
   },
   "source": [
    "### For this analysis, I will use activity with counts more than 24 and below 70."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd156fc3-5360-4055-b541-f6bdfd1bfe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Morover, I will sample a subset of exercise \n",
    "keep_exercise_list = []\n",
    "for k,v in value_counts_by_exercise.items():\n",
    "    if (v > 24) and (v<70): \n",
    "        keep_exercise_list.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91c8185-fd30-44b0-905e-14b98aea0c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_file = total_file[total_file['label'].isin(keep_exercise_list)]\n",
    "len(subset_file.global_file_idx.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c622fd5d-597a-4276-bfa1-b1f33222622a",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9174008-48d0-4de0-a074-5f62b0193020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets check the average number of sample points per recording\n",
    "counts_per_sample = subset_file.groupby('global_file_idx').count()\n",
    "print(f\"Min:{counts_per_sample.min()[0]}, Max:{counts_per_sample.max()[0]}, Mean:{counts_per_sample.mean()[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7f4069-d8c1-469b-a169-8ecb49160b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave only records with at least 160 data points\n",
    "globalidx_list = counts_per_sample[counts_per_sample['label']>=160].index.tolist()\n",
    "second_subset_file = subset_file[subset_file['global_file_idx'].isin(globalidx_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d443db6a-3404-481c-bd74-d378c76ce1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_per_sample = second_subset_file.groupby('global_file_idx').count()\n",
    "print(f\"Min:{counts_per_sample.min()[0]}, Max:{counts_per_sample.max()[0]}, Mean:{counts_per_sample.mean()[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2970cde7-191b-4f05-9175-27cdac08f9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_subset_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0473aa80-25e0-4997-b6c5-2a6bd2b7b107",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = second_subset_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d254bd-7f73-48ef-a0d1-e44e104f6d6d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Split Train And Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ab144c-cd6d-4337-ab99-76b8e34a0889",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(10)\n",
    "# Split by users into train and val\n",
    "unique_user_ids = final_df.user_id.unique()\n",
    "print(f'There are total of {len(unique_user_ids)} number of users')\n",
    "random.shuffle(unique_user_ids)\n",
    "\n",
    "# 70% train 30% test\n",
    "train_len = int(len(unique_user_ids) * 0.7)\n",
    "train_ids = unique_user_ids[:train_len]\n",
    "test_ids = unique_user_ids[train_len:]\n",
    "\n",
    "print(f'Train User ID LEN: {len(train_ids)} number of users')\n",
    "print(f'Test User ID LEN: {len(test_ids)} number of users')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776b3f58-fb36-45d3-aa6c-8c66baebf9aa",
   "metadata": {},
   "source": [
    "## From the train_ids, I will split the ids into 5 for 5-fold cross validation training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805ad3d2-6fba-4dd3-9a4d-65e00fb09c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d579e8-314b-4281-abc1-ee8d9b51a8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fivefold_map = {}\n",
    "fold_num = 0\n",
    "for train_id in train_ids:\n",
    "    fivefold_map[train_id] = fold_num\n",
    "    fold_num +=1\n",
    "    if fold_num == 5:\n",
    "        fold_num=0\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e32019-4c97-4b5d-b23f-791026525558",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Lets check the number of unique activities in both Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4a453d-fe13-43d1-a639-76c24185db15",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = final_df[final_df['user_id'].isin(train_ids)]\n",
    "test_df = final_df[final_df['user_id'].isin(test_ids)]\n",
    "# The number of classes in both train and test should equal\n",
    "assert(len(train_df.exercise_idx.unique()) == len(test_df.exercise_idx.unique()))\n",
    "print(f\"Number of unique activities in both train and test are equal, the number of unique activities are: {len(train_df.exercise_idx.unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4facf8f9-7420-45e4-ac21-3071d24160c6",
   "metadata": {},
   "source": [
    "### Lets chunk the final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231001d1-93c6-4de1-8a90-0a5713b24eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.loc[0,\"user_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7b7e94-fa74-490b-ae10-85c2c5538985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunkdata(df,non_overlap_step,maximum_num_chunk_per_global_file=5, window_size=160):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        non_overlap_step: how much are we going to non_overlap the data when chunking? if 40 is given, it would index [0: 160, 40: 200, 80: 220]\n",
    "        if you do not want to overlap at all, non_overlap_step == window_size\n",
    "        maximum_num_chunk_per_global_file:  how many chunks are we going to get at most per global file?\n",
    "        window_size: The size of the chunk\n",
    "    \"\"\"\n",
    "    \n",
    "    assert window_size >=160\n",
    "    assert non_overlap_step <= window_size\n",
    "    feature_columns = ['acc_x', 'acc_y', 'acc_z', 'gy_x', 'gy_y', 'gy_z','user_id']\n",
    "    label_columns = ['user_id','recording_idx', 'exercise_idx', 'global_file_idx', 'label']\n",
    "\n",
    "    \n",
    "    df_train_lists = []\n",
    "    df_label_lists = []\n",
    "    \n",
    "    df_copy = df.copy()\n",
    "    GLOBAL_ID = 0\n",
    "    \n",
    "    for user_id in tqdm(df_copy.user_id.unique()):\n",
    "        for global_file_id in df_copy.loc[df_copy['user_id']==user_id]['global_file_idx'].unique():\n",
    "            train_data = df_copy.loc[(df_copy['user_id']==user_id)&(df_copy['global_file_idx']==global_file_id),feature_columns]\n",
    "            train_label = df_copy.loc[(df_copy['user_id']==user_id)&(df_copy['global_file_idx']==global_file_id),label_columns]\n",
    "            for i in range(len(train_data)):\n",
    "                if i*non_overlap_step+window_size <=len(train_data):\n",
    "                    data_chunk = train_data.iloc[i*non_overlap_step:i*non_overlap_step+window_size,:].copy()\n",
    "                    data_chunk['GLOBAL_ID'] = GLOBAL_ID\n",
    "                    label_chunk = train_label.iloc[0,:].copy()\n",
    "                    label_chunk['GLOBAL_ID'] = GLOBAL_ID\n",
    "                    GLOBAL_ID+=1\n",
    "                    df_train_lists.append(data_chunk)\n",
    "                    df_label_lists.append(label_chunk)\n",
    "                if i+1 ==maximum_num_chunk_per_global_file:\n",
    "                    break\n",
    "\n",
    "                \n",
    "    data = pd.concat(df_train_lists).reset_index(drop=True)\n",
    "    label = pd.concat(df_label_lists,axis=1).T.reset_index(drop=True)\n",
    "    \n",
    "    return data,label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7daf790-9b99-424f-82fc-e2096d49b879",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data,label = chunkdata(final_df,non_overlap_step=160, maximum_num_chunk_per_global_file=5, window_size=160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b509800b-77b5-4513-8bdd-4327a2c3b2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[data['user_id'].isin(train_ids)]\n",
    "train_label = data[data['user_id'].isin(train_ids)]\n",
    "test_data = data[data['user_id'].isin(test_ids)]\n",
    "test_label = data[data['user_id'].isin(test_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088d8792-ee9c-46a4-8e6b-29e594d09cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b749c14b-d01c-4956-b4c5-9a7804baa532",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ad6182-3a8d-47dc-aa45-43fad67f3abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label['CV_VAL'] = train_label.copy().user_id.map(fivefold_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c72a851-f0a1-40d5-a0bb-49b831e70ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75ce5f6-e96a-44fe-9b98-2b254b65fd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155fc634-5210-4f6f-ace9-0fb509f1382b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('processed_data/MS_train_features_chunk160_window160.pkl', 'wb') as f:\n",
    "    pickle.dump(train_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae9f775-e098-4c26-bc66-47a1777ea235",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('processed_data/MS_train_labels_chunk160_window160.pkl', 'wb') as f:\n",
    "    pickle.dump(train_label, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1677feda-bdb7-4bba-ace6-d31c2ba8d6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('processed_data/MS_test_features_chunk160_window160.pkl', 'wb') as f:\n",
    "    pickle.dump(test_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dd4a16-5ce2-47b3-89ae-5ac32c015514",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('processed_data/MS_test_labels_chunk160_window160.pkl', 'wb') as f:\n",
    "    pickle.dump(test_label, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gilon",
   "language": "python",
   "name": "gilon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
